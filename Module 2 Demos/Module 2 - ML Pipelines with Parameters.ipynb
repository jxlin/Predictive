{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.sql.Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Load and parse the data file.\n",
    "val input = sc.textFile(\"/asnumbers.csv\")\n",
    "// The above is an Array[String] which is not what we need. We need a LabeledPoint \n",
    "val data  = input.map { line => \n",
    "    val lineSplit = line.split(',')\n",
    "    val values = Vectors.dense(lineSplit.take(12).map(_.toDouble))\n",
    "    (lineSplit(13).toDouble, values)\n",
    "}\n",
    "val splits = data.randomSplit(Array(0.7, 0.3))\n",
    "val (trainingRDD, testRDD) = (splits(0), splits(1))\n",
    "val training = trainingRDD.toDF(\"label\", \"features\")\n",
    "val test = testRDD.toDF(\"label\", \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Create the model - which is an estimator and find out the params are then set the number of iterations and regularization\n",
    "val lr = new LogisticRegression()\n",
    "println(\"LogisticRegression parameters:\\n\" + lr.explainParams() + \"\\n\")\n",
    "lr.setMaxIter(50).setRegParam(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Display how the model was built using params after calling fit on the estimator method \n",
    "val model = lr.fit(training)\n",
    "println(\"Params: \" + model.parent.extractParamMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Can change any of the algo params and override the defaults in the param map\n",
    "val map1 = ParamMap(lr.maxIter -> 100).put(lr.regParam -> 0.01, lr.threshold -> 0.4)\n",
    "val map2 = ParamMap(lr.predictionCol -> \"pred\") \n",
    "val sum = map1 ++ map2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val modelupdate = lr.fit(training, sum)\n",
    "println(\"Update to params: \" + modelupdate.parent.extractParamMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modelupdate.transform(test).select(\"features\", \"label\", \"probability\", \"pred\").collect().foreach { \n",
    "    case Row(features: Vector, label: Double, probability: Vector, pred: Double) =>\n",
    "        println(s\"($features, $label) -> probability=$probability, prediction=$prediction\")\n",
    "  }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
