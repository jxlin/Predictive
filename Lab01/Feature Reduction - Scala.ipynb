{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vec: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = MapPartitionsRDD[7] at map at <console>:32"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.PCA\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "import sqlContext.implicits._\n",
    "val input = sc.textFile(\"wasb:///iris-multiclass.csv\")\n",
    "val vec = input.map(line => line.split(\",\")).map(mapped => Vectors.dense(Array(mapped(0).toDouble, mapped(1).toDouble, mapped(2).toDouble, mapped(3).toDouble)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res1: Array[org.apache.spark.mllib.linalg.Vector] = Array([5.1,3.5,1.4,0.2], [7.0,3.2,4.7,1.4], [6.3,3.3,6.0,2.5], [4.9,3.0,1.4,0.2], [6.4,3.2,4.5,1.5])"
     ]
    }
   ],
   "source": [
    "vec.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df: org.apache.spark.sql.DataFrame = [features: vector]"
     ]
    }
   ],
   "source": [
    "import sqlContext.implicits._\n",
    "val data = vec.collect()\n",
    "val df = sqlContext.createDataFrame(data.map(Tuple1.apply)).toDF(\"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pcsModel: org.apache.spark.ml.feature.PCAModel = pca_782ddc0e26b1"
     ]
    }
   ],
   "source": [
    "val pca = new PCA().setInputCol(\"features\").setOutputCol(\"outfeatures\").setK(2)\n",
    "val pcsModel = pca.fit(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res2: Array[org.apache.spark.sql.Row] = Array([[-2.827135972679015,-5.641331045573371]], [[-6.796137685628032,-6.0001629169419175]], [[-8.043070078222522,-5.3028814941686795]], [[-2.795952482148833,-5.145166883252961]], [[-6.44375385076588,-5.633921820642639]])"
     ]
    }
   ],
   "source": [
    "val pcaDF = pcsModel.transform(df)\n",
    "val result = pcaDF.select(\"outfeatures\")\n",
    "result.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comps: org.apache.spark.mllib.linalg.Matrix = \n",
      "-0.36158967738144987  -0.6565398832858255  \n",
      "0.0822688898922141    -0.7297123713265027  \n",
      "-0.8565721052905283   0.17576740342864972  \n",
      "-0.3588439262482152   0.07470647013503745"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.mllib.linalg.distributed.RowMatrix\n",
    "val matrix = new RowMatrix(vec)\n",
    "val comps = matrix.computePrincipalComponents(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
