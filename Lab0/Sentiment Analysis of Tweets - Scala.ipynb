{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SparkContext as 'sc'\n",
      "Creating SqlContext as 'sqlContext'\n",
      "Creating HiveContext as 'hiveContext'\n",
      "import org.apache.spark.sql.Row"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "import org.apache.spark.ml.feature.{HashingTF, Tokenizer}\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}\n",
    "import org.apache.spark.mllib.linalg.Vector\n",
    "import org.apache.spark.sql.Row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: org.apache.spark.sql.DataFrame = [id: int, label: double, source: string, text: string]"
     ]
    }
   ],
   "source": [
    "case class Tweets(id: Int, label: Double, source: String, text: String)\n",
    "val training = sc.textFile(\"wasb:///training-tweets.csv\").zipWithIndex().filter(_._2 > 0).map(line => line._1.split(\",\")).map(tw => Tweets(tw(0).toInt, tw(1).toDouble, tw(2), tw(3))).toDF() \n",
    "\n",
    "val test = sc.textFile(\"wasb:///test-tweets.csv\").zipWithIndex().filter(_._2 > 0).map(line => line._1.split(\",\")).map(tw => Tweets(tw(0).toInt, tw(1).toDouble, tw(2), tw(3))).toDF()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline: org.apache.spark.ml.Pipeline = pipeline_b1448efec087"
     ]
    }
   ],
   "source": [
    "val tokenizer = new Tokenizer().setInputCol(\"text\").setOutputCol(\"words\")\n",
    "val hashingTF = new HashingTF().setInputCol(tokenizer.getOutputCol).setOutputCol(\"features\")\n",
    "val lr = new LogisticRegression().setMaxIter(10)\n",
    "val pipeline = new Pipeline().setStages(Array(tokenizer, hashingTF, lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid: Array[org.apache.spark.ml.param.ParamMap] = \n",
      "Array({\n",
      "\thashingTF_6098835c3dcb-numFeatures: 10,\n",
      "\tlogreg_d34c68eb22c3-regParam: 0.1\n",
      "}, {\n",
      "\thashingTF_6098835c3dcb-numFeatures: 10,\n",
      "\tlogreg_d34c68eb22c3-regParam: 0.01\n",
      "}, {\n",
      "\thashingTF_6098835c3dcb-numFeatures: 20,\n",
      "\tlogreg_d34c68eb22c3-regParam: 0.1\n",
      "}, {\n",
      "\thashingTF_6098835c3dcb-numFeatures: 20,\n",
      "\tlogreg_d34c68eb22c3-regParam: 0.01\n",
      "}, {\n",
      "\thashingTF_6098835c3dcb-numFeatures: 100,\n",
      "\tlogreg_d34c68eb22c3-regParam: 0.1\n",
      "}, {\n",
      "\thashingTF_6098835c3dcb-numFeatures: 100,\n",
      "\tlogreg_d34c68eb22c3-regParam: 0.01\n",
      "})"
     ]
    }
   ],
   "source": [
    "val paramGrid = new ParamGridBuilder()\n",
    "paramGrid.addGrid(hashingTF.numFeatures, Array(10, 20, 100))\n",
    "paramGrid.addGrid(lr.regParam, Array(0.1, 0.01))\n",
    "val grid = paramGrid.build()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res6: crossvalidator.type = cv_5ab3eec6a398"
     ]
    }
   ],
   "source": [
    "val crossvalidator = new CrossValidator()\n",
    "crossvalidator.setEstimator(pipeline)\n",
    "crossvalidator.setEvaluator(new BinaryClassificationEvaluator)\n",
    "crossvalidator.setEstimatorParamMaps(grid)\n",
    "crossvalidator.setNumFolds(6) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: org.apache.spark.ml.tuning.CrossValidatorModel = cv_5ab3eec6a398"
     ]
    }
   ],
   "source": [
    "val model = crossvalidator.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res8: Double = 57.01943844492441"
     ]
    }
   ],
   "source": [
    "val modelem = model.transform(test).select(\"id\", \"label\", \"text\", \"probability\", \"prediction\")\n",
    "modelem.collect().foreach { case Row(id: Int, label : Double, text: String, probability: Vector, prediction: Double) =>\n",
    "    println(s\"($id, $text) --> prob=$probability, prediction=$prediction\")\n",
    "  }\n",
    "(modelem.filter(\"label = prediction\").count().toDouble / modelem.count().toDouble) * 100D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res9: Array[Double] = Array(0.595102078409595, 0.5960110108059493, 0.6160856959751109, 0.618246455638469, 0.6709262902666684, 0.673400823759253)"
     ]
    }
   ],
   "source": [
    "model.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.ml.tuning.TrainValidationSplit"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.tuning.TrainValidationSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: org.apache.spark.sql.DataFrame = [id: int, label: double, source: string, text: string]\n",
      "test: org.apache.spark.sql.DataFrame = [id: int, label: double, source: string, text: string]"
     ]
    }
   ],
   "source": [
    "case class Tweets(id: Int, label: Double, source: String, text: String)\n",
    "val tweets = sc.textFile(\"wasb:///training-tweets.csv\").zipWithIndex().filter(_._2 > 0).map(line => line._1.split(\",\")).map(tw => Tweets(tw(0).toInt, tw(1).toDouble, tw(2), tw(3))).toDF()\n",
    "val Array(training, test) = tweets.randomSplit(Array(0.9, 0.1), seed = 11L)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res13: trainValidationSplit.type = tvs_78b1c8a103d7"
     ]
    }
   ],
   "source": [
    "val trainValidationSplit = new TrainValidationSplit()\n",
    "trainValidationSplit.setEstimator(pipeline)\n",
    "trainValidationSplit.setEvaluator(new BinaryClassificationEvaluator)\n",
    "trainValidationSplit.setEstimatorParamMaps(grid)\n",
    "trainValidationSplit.setTrainRatio(0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: org.apache.spark.ml.tuning.TrainValidationSplitModel = tvs_78b1c8a103d7"
     ]
    }
   ],
   "source": [
    "val model = trainValidationSplit.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+\n",
      "|            features|label|prediction|\n",
      "+--------------------+-----+----------+\n",
      "|(100,[0,4,5,13,40...|  1.0|       0.0|\n",
      "|(100,[0,1,7,11,39...|  1.0|       0.0|\n",
      "|(100,[0,33,97],[4...|  0.0|       0.0|\n",
      "|(100,[0,1,5,12,16...|  0.0|       0.0|\n",
      "|(100,[0,1,5,6,14,...|  1.0|       0.0|\n",
      "|(100,[0,4,5,50],[...|  0.0|       0.0|\n",
      "|(100,[0,5,7,13,28...|  0.0|       0.0|\n",
      "|(100,[0,7,9,23,52...|  1.0|       0.0|\n",
      "|(100,[0,1,6,11,12...|  1.0|       0.0|\n",
      "|(100,[0,17,45,56,...|  0.0|       0.0|\n",
      "|(100,[0,9,16,18,2...|  0.0|       0.0|\n",
      "|(100,[0,1,14,25,2...|  0.0|       0.0|\n",
      "|(100,[0,4,6,14,47...|  0.0|       0.0|\n",
      "|(100,[0,23,39,43]...|  0.0|       0.0|\n",
      "|(100,[0,1,30,39,6...|  0.0|       1.0|\n",
      "|(100,[0,10,16,27,...|  1.0|       1.0|\n",
      "|(100,[34,43,50,66...|  0.0|       0.0|\n",
      "|(100,[0,5,8,17,26...|  1.0|       0.0|\n",
      "|(100,[0,1,6,7,9,1...|  1.0|       1.0|\n",
      "|(100,[0,8,11,22,2...|  1.0|       1.0|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "model.transform(test).select(\"features\", \"label\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res15: Array[Double] = Array(0.610366086135461, 0.6117928406115689, 0.6299418873072471, 0.6308518528447218, 0.6853254281008085, 0.6863389003565411)"
     ]
    }
   ],
   "source": [
    "model.validationMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
