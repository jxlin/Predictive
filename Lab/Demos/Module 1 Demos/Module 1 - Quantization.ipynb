{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.Bucketizer\n",
    "import org.apache.spark.sql.DataFrame\n",
    "// Create the buckets or bins here\n",
    "val splits = Array(Double.NegativeInfinity, 8.0, 12.0, 15.0, Double.PositiveInfinity)\n",
    "// Setup the data and parallelize across Spark\n",
    "val temp = Array((1, 10.2), (2, 17.1), (3, 9.6), (4, 5.0), (5, 3.4))\n",
    "val rainfall = sc.parallelize(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// convert to a dataframe\n",
    "val df = sqlContext.createDataFrame(rainfall).toDF(\"id\", \"rainfall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val bucketizer = new Bucketizer()\n",
    "bucketizer.setInputCol(\"rainfall\")\n",
    "bucketizer.setOutputCol(\"discrainfall\")\n",
    "bucketizer.setSplits(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Transform original data into its bucket index.\n",
    "val bucketedData = bucketizer.transform(df)\n",
    "bucketedData.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
