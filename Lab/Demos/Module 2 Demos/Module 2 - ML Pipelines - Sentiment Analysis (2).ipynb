{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.feature.{HashingTF, Tokenizer}\n",
    "import org.apache.spark.mllib.linalg.Vector\n",
    "import org.apache.spark.sql.Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "case class Tweets(id: Int, label: Double, source: String, text: String)\n",
    "val training = sc.textFile(\"/training-tweet.csv\").zipWithIndex().filter(_._2 > 0).map(line => line._1.split(\",\")).map(tw => Tweets(tw(0).toInt, tw(1).toDouble, tw(2), tw(3))).toDF() \n",
    "val test = sc.textFile(\"/test-tweets.csv\").zipWithIndex().filter(_._2 > 0).map(line => line._1.split(\",\")).map(tw => Tweets(tw(0).toInt, tw(1).toDouble, tw(2), tw(3))).toDF() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
    "val tokenizer = new Tokenizer().setInputCol(\"text\").setOutputCol(\"words\")\n",
    "val hashingTF = new HashingTF().setNumFeatures(1000).setInputCol(tokenizer.getOutputCol).setOutputCol(\"features\")\n",
    "val lr = new LogisticRegression().setMaxIter(10).setRegParam(0.01)\n",
    "val pipeline = new Pipeline().setStages(Array(tokenizer, hashingTF, lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Fit the pipeline to training documents.\n",
    "val model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val modelem = model.transform(test).select(\"id\", \"label\", \"text\", \"probability\", \"prediction\")\n",
    "modelem.collect().foreach { \n",
    "\tcase Row(id: Int, label: Double, text: String, probability: Vector, prediction: Double) => \n",
    "\tprintln(s\"($id, $text) --> prob=$probability, prediction=$prediction, label=$label\")\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(modelem.filter(\"label = prediction\").count().toDouble / modelem.count().toDouble) * 100D"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
