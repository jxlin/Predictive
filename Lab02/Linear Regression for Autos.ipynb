{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SparkContext as 'sc'\n",
      "Creating SqlContext as 'sqlContext'\n",
      "Creating HiveContext as 'hiveContext'\n",
      "import org.apache.spark.mllib.feature.StandardScaler"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.regression.LinearRegressionModel\n",
    "import org.apache.spark.mllib.regression.LinearRegressionWithSGD\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "import org.apache.spark.mllib.feature.StandardScaler \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featurevector: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[2] at map at <console>:28"
     ]
    }
   ],
   "source": [
    "val input = sc.textFile(\"wasb:///autos.csv\")\n",
    "\n",
    "val featurevector  = input.map { line =>\n",
    "    val lineSplit = line.split(',')\n",
    "    val featureArr = lineSplit.slice(1, 8).map(_.toDouble)\n",
    "    val values = Vectors.dense(featureArr)\n",
    "    LabeledPoint(lineSplit(0).toDouble, values)\n",
    "}.persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaledData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[5] at map at <console>:31"
     ]
    }
   ],
   "source": [
    "val scaler = new StandardScaler().fit(featurevector.map(x => x.features))\n",
    "val scaledData = featurevector.map(x => LabeledPoint(x.label,scaler.transform(Vectors.dense(x.features.toArray))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[6] at randomSplit at <console>:33\n",
      "test: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[7] at randomSplit at <console>:33"
     ]
    }
   ],
   "source": [
    "val allData = scaledData.randomSplit(Array(0.7, 0.3), seed = 11L)\n",
    "val (training, test) = (allData(0), allData(1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res6: algorithm.optimizer.type = org.apache.spark.mllib.optimization.GradientDescent@33d02597"
     ]
    }
   ],
   "source": [
    "val numIterations = 100\n",
    "val stepSize = 0.001\n",
    "val algorithm = new LinearRegressionWithSGD()\n",
    "algorithm.setIntercept(true)\n",
    "algorithm.optimizer.setNumIterations(numIterations)\n",
    "algorithm.optimizer.setStepSize(stepSize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: org.apache.spark.mllib.regression.LinearRegressionModel = org.apache.spark.mllib.regression.LinearRegressionModel: intercept = 1.0453147863792198, numFeatures = 7"
     ]
    }
   ],
   "source": [
    "val model = algorithm.run(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res7: Array[(Double, Double)] = Array((18.0,21.73829610544691), (16.0,21.847308715274426), (15.0,21.5213273965266), (15.0,21.648102228681044), (14.0,21.432607843528572), (15.0,21.58980829309481), (24.0,22.086207696159754), (25.0,22.40982931254288), (26.0,21.874441339602015), (19.0,22.073750795258192), (14.0,22.140757973615873), (14.0,22.266296901876675), (12.0,22.179968212493833), (19.0,22.33109945991472), (23.0,22.091506861093198), (30.0,22.85532175199785), (23.0,23.551281176582325), (13.0,22.44705026736736), (17.0,22.34540231057206), (13.0,22.616024568736798))"
     ]
    }
   ],
   "source": [
    "val valuesAndPreds = test.map { point =>\n",
    "  val prediction = model.predict(point.features)\n",
    "  (point.label, prediction)\n",
    "}\n",
    "valuesAndPreds.take(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Mean Squared Error = 50.91870946643104"
     ]
    }
   ],
   "source": [
    "val MSE = valuesAndPreds.map{case(v, p) => math.pow((v - p), 2)}.mean()\n",
    "println(\"training Mean Squared Error = \" + MSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
